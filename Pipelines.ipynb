{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines \n",
    "\n",
    "## Introduction \n",
    "Pipelines are a simple way to keep data preprocessing and modeling code organized. It's purpose is to act as a bundle which takes the preprocessing and modeling steps into one bundle as if it were a single step. <br/> <br/>\n",
    "Implementing piplelines allows our code to appear cleaner (we won't need to manually keep track of training and validation data at each step), have fewer bugs (fewer opportunities to misapply for forget a step). Additionally, it is also easier to productionize and also help provide more options for model validation (we'll see this more in **cross-validation**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Pipeline \n",
    "\n",
    "To set up a pipeline all we'll do is set up all the preprocessing methods and then combine them into a **Column Transformer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "# constant changes the values to use fill_value which is 0 if left to default\n",
    "numerical_transformer = SimpleImputer(strategy='constant') \n",
    "\n",
    "# Preprocessing for categorical data\n",
    "# using most_frequent replaces empty values with the most frequent value, in this case a string then\n",
    "# converts them using a onehot encoding method \n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the preprocessor section has been setup, we can define the model type in which we wish to fit to data to. Afterwards, we can combine the preprocessor with the model to form our pipeline. Once that process is complete, we can fit and validate our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model using some method such as mean absolute error (MAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
